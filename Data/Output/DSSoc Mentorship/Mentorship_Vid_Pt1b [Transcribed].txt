So of course, often when we talk about data ethics or the impact of datafication to society, especially if you talk about it within the context of the global south or in the Philippines, most of the examples of datafied harm really comes from the global north, right? For example, the one that we just saw. And while it's true that the datafication is not at the level of maturity here in the Philippines, in most of the global south, all of these harms that have already been studied, identified, and are being called out in more advanced or rather more mature datafied societies should then inform us as we develop our own datafication maturity, as we develop the data science here in the Philippines, to already see these red flags and not commit those same set of mistakes, right? So like for some of you or for some of you that will eventually, for example, work in the government with like your data science skill sets, you know that then you then know that a number of the initiatives that you're trying to do are likely going to replicate initiatives that have been done already before in other countries. And then it is your responsibility to see, well, what tamo ba yung ginawan nila? Was it actually ethical? What were the harms that were made? So that you can actually mitigate it accordingly. So that's one point from that video. The second point from that video talking about the data science maturity in the Philippines, it's the fact of the matter that we will never reach a certain level of data science maturity in the Philippines if we don't have enough data. And the reality of which, especially if you're looking at data science within the, for the government or with the government, your data will always come from records. Your records are documents. Our government is batshit bad at document management or records keeping. And so you can, it would be very, we are still at that particular juncture that the jump from records and documents to data is hard to make precisely because the foundation of which, like if you're trying to come up with a training data actually does not exist. Or if it does exist, they're still at a particular form of records and documents that needs to be contended with. And even that, right, even once we've done that, then you can say, so then let's just go through an extensive digitization endeavor, right? To actually create or process or extract data. At that juncture, then you really have to still think ethically of how you're going to go about it and be critical in terms of how those records and documents were created in the first place, before you even consider them as data that you're going to then process and play around with. So those are the two key things that I wanted to point out with regards to that video in regards to that specific case study as well. That video also pointed out to one possible ethical approach to data ethics. I'm not here to do, rather this particular mentoring session is not meant to convince you to subscribe to any of these ethical approaches, but rather the goal really is for you to start thinking about data ethics. And the best way to do it really is to think with people who have been doing this for quite some time. So the video talks about situated data science, or that when you develop, for example, design your database or develop your training data, or when you attempt to design your algorithm or deploy your machine learning techniques, the goal here really is to pursue what in the human computer interaction world calls user centered design, right? That is not to design for but rather to design with. So what does this mean? This means, for example, that what we're trying to do is that we have to lead our data initiatives, rather our data initiatives need to be led by user needs, and not by the technology. Another way of framing this is rather than saying, how can we use machine learning to do x, the right framing of the question should be, what problems do our users have? And could machine learning solve these problems in a unique and or efficient way? So why the two framing seems to be similar, rhetorically, epistemologically, they're actually very, very different, right? Because the user centered design or user centered approach really focuses on identifying the need and figuring out if the technology actually fits that need, as opposed to the other way around, right? A couple of months from now, a couple of years from now, you'll be like, I'm a data scientist, I can solve the ills, problems of the world, because I have this particular tool. If you approach, if your lens throughout social engagement follows that line of thinking, that you're then you're by default being technocentric, right? And you're not being user centered. So that is, if one way of possibly going and exploring a more ethical approach to data is centering the users, and maybe not centering the technology, or in this case, the data. But sometimes the best thing to do is actually to say no. So there's you, you, there's a link here about this particular framing and positionality, drawing from feminist epistemologies and approaches as well, right? So rather than saying that rather than rather than stating and pursuing data as a solution to everything, feminist approaches generally focuses on centering care, responsibility, and diminishing harm. And the best sometimes the best way to actually do this is through acts of refusal, together with a clear set of commitments, right? So what do we refuse to do? Or what do we refuse to believe in? What do we refuse to subscribe to? As equally important as what do we commit to? What do we profess to? What do we pursue? Right? And so this feminist data manifesto is a series of manifestations of refusals and commitments. I'm not going to go through each one of them, though I highly recommend that you go visit the manifestos and and look at it yourself. Though I would read one of my favorite lines from the feminist data manifesto that says, we refuse to operate under the assumption that risk and harm associated with data practices can be bounded to mean the same thing for everyone, everywhere at every time. We commit to acknowledging how historical systemic patterns of violence and exploitation produce differential vulnerabilities for communities. What this particular manifestation articulates is that, well, one of the things rather that it articulates is that data good, what is considered good for one community can easily be considered as harm for another community. Right? Often, it is very difficult, if not impossible, to find absolute, absolute goods. And so the manifestation here and the commitment here is to acknowledge that good and harm has different registers and different impact and different scales for different communities. And we have to be very cognizant of that when we make a decision with regards to the design of and the design, the pursuance, the deployment of data to solve the problems of the communities we purport to serve. Yasenov Sheila also talks about technology, the tensions between technology of fulbris and technology of humility. And this is, this is along the same lines of what I was pertaining to earlier. Yasenov speaks of technologies of fulbris as those predictive models like risk assessment, cost benefit analysis, climate modeling, that are designed on the whole to facilitate management and control, even in areas of high uncertainty. We see this, for example, when data scientists says, Oh, yeah, we can figure that out. We'll run the numbers for you. And we're correct. And we're right. And so they situate or rather they position data at the level of fulbris. Such predictive methods, however, Yasenov argues, have three shortcomings. First, they deny uncertainty and ignorance, which often is the case for social ills and social problems, given its complexity. Second, they short circuit the moral dimension of new technological developments, precisely because these technologies are never neutral and always are implicated in politics. Third, they do not address the need for profound social learning from, for example, errors and catastrophes. Because if you operate within the level of fulbris, then you will never consider that you've made a mistake. Or if you've made a mistake, you will always chuck it and say, Nah, that's just part and parcel of like, that's the cost of like, datafication. Sometimes there are errors. But you know, that's just that it is what it is, right. As an alternative, she proposed the development of what she calls technologies of humility, which addresses issues, all of these issues that was mentioned, precisely because it looks at framing, vulnerability, distribution and learning. What if we become then data scientists driven by humility, as opposed to hubris. And finally, there is an initiative here back home, there's a website, you can visit it at ethics.ph. This is called data ethics.ph. We're in the, it's an ongoing set of conversations and case studies of data science applications through different contexts locally in the country. There's also conversation, these, the website also hosts conversations between established and emerging data scientists, and the ethical quandaries of the work that they do in the specific domains that they operate in. The website states, for example, that they believe that data rights are human rights, and that social problems are data problems. So as young and as emerging as this particular initiative is, there are already discussions that are going on locally here in the Philippines. And similarly, this boot camp, I would urge that anyone who wants to become a data scientist, as much of the participants in this boot camp plan to be, your starting point, your end point, and embedded throughout all your learning should always be this notion of data ethics, because we don't want to be situated. And we don't want to be, we don't want to find our communities, ourselves, in our society at large, to be at the same set of issues, same set of concerns, and same set of harms and violence that are now being experienced by communities with quote, unquote, more mature datafication. Also, having these localized conversations enables us to both figure out new ways of doing, right, pursue new ways of developing and acting in terms of data work. But at the same time, it enables us to stand and resist the harmful impact of datafication, specifically from, for example, the global north, right? When Facebook or Google is enacting harm of us through data from our own data, if we then have a rich understanding and commitment to data ethics, we'll be able to say, no, shut that down, and be able to also offer new imaginaries, new perspectives of what an ethical datafied world can actually look like. And so I'll end by then talking about how can we pursue these ethical goals, both as a group, as an organization, and also as individuals. So any pursuance of ethics, I believe, should be founded on a number of things. One, I do believe that it's a multi, inter, and transdisciplinary task, meaning we will never figure out data ethics, if we're only focusing on learning skill sets in terms of the technologies that we use, right? Figuring out, am I going to use R? What coding language or languages should actually, should I actually learn? None of those courses will actually lead you to an ethical exploration of your data practices, right? So this means being open to other disciplines that actually have been contending with notions of ethics and notions of data, right? Whether you're looking at courses from anthropology or library and information science or philosophy or sociology or communication and media studies, a number of these disciplines are immersing themselves within these and have immersed themselves in these particular set of issues. As data scientists, it is our responsibility to then be open to these disciplines to inform the work that we do and also inform them at the same time in return, right? So there's a lot of critics of data science, and sometimes maybe they don't actually understand how data science works, right? And so to have those conversations across disciplines is necessary to pursue this ethical goal. It also means that ethics is not like a one-time thing, right? It's not a thing that like, you'll figure things out and you'll hold on to this, right? It's a constant state of being, it changes based on context, it changes on who you engage with, and so on and so forth. And so then it means that it is an ongoing process with others. This is also connected to the first point. For example, if you're a data scientist and you find yourselves developing a, I don't know, using data for the criminal justice system, right? Then you should learn how the criminal justice system works, right? You should then learn about the inequities perpetrated by the criminal justice system, right? If for example, you are developing recommender systems for say, a streaming service, then you should be able to understand or have appreciation for, a critical appreciation and critical understanding of audience studies or how media consumption actually works, right? So you're not just a data scientist in a bubble, you're always a data scientist within the context of the domain and discipline that you exist in. So in my case, like I work in the human rights space, so I need to know the ins and outs and the politics and the nuances of how human rights is pursued, perceived, pursued, how it is perceived and pursued, and how datafication actually impacts that and vice versa as well. So it's not only enough for you to learn all of these technical skills, it is you need for you to be a good and ethical data scientist, you need to understand the domains that you operate in as well, or you wish to operate in as well. Because finally, being students of UP, at the end of the day, you're going to be data scientists with the Tatak UP. An ethical goal means a commitment or rather a committed engagement with the communities we purport to serve. We'll come back to that. Who are the people who are the communities that we wish to serve? What are our commitments to them? Are we actually providing space to hear them out and have conversations with them? Are we designing with them and not just for them? So data ethics, data is never raw. I hope that this provides you a good introduction and a good overview, if not a provocation really for you, for all of you to start not only thinking but really embedding data ethics in your journey towards becoming a full-fledged data scientist in service of the community.
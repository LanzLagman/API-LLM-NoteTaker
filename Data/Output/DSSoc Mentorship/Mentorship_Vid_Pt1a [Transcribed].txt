So for this particular talk, as I talk about data ethics, I look at data as both document and technology. And it speaks to a number of my interdisciplinary commitments and backgrounds. I'm an information scientist, an archivist by trade. And so I specifically see the world as being governed by and mediated, governed and mediated by documents. We operate in this world as documented and documented individuals and documentarians as well. And I argue when I speak of data that the logics of the document extends to the same logics of data. Probably a slightly different in scale, probably slightly different in reach, but nonetheless, the registers and logic of how documents mediate applies as well. But I also look at data as a form of technology or as a mechanism of technology. Any discussion with regards to ethics and the introduction, deployment and design of technology, arguably can also be applied to discussions and engagements of data. So that's my positionality in terms of my perspectives when it comes to data. My commitments when it comes to data is specifically tied to notions or rather the pursuance of human rights. As mentioned in the introduction, currently I work as a documentalist for an international NGO based in Geneva. What we do at this organization called Euridox is we basically develop technologies in support of human rights defenders across the globe. Specifically, we develop databases and set up information infrastructures for human rights organizations, institutions and defenders, community organizers and activists from Manila all the way to Chile and everything in between with a specific commitment to defenders in the global South. So my day-to-day work really is talking to human rights defenders and creating data models and data structures, setting up databases with them, helping them clean their data, importing them accordingly. For those with robust enough databases and with duly populated data, we then go into discussions in terms of analyzing these data, creating narratives, developing human rights campaigns, a data-centric and database human rights campaign. So my relationship with data has always been within this context of human rights. So why do we then have to talk about data ethics? I don't think I need to speak to the value and importance and the impact that data has in different parts of our lives. Most of you here are data scientists or are emerging or want to become data scientists precisely because you see its value and its power. And I won't disagree that. As I told you, I work in the data and human rights space. By default, I believe that data can be used for good. But the impetus for talking about data ethics is precisely based on the intersection or rather the relationship between data algorithms and data-driven practice. That now more than ever is becoming more ubiquitous, integrating in different aspects of our personal and social lives as well. Floridi Luciano and Tadeo Maria Soria wrote in what is data ethics, this line or rather this phrase that I believe summarizes well this impetus for an ethical practice around data. They state, the extensive use of increasingly more data, often personal, if not sensitive, and the growing reliance on algorithms to analyze them in order to shape choices and to make decisions, as well as the gradual reduction of human involvement or even oversight over many automatic processes post pressing issues of fairness, responsibility and respect of human rights among others. Day to day we see that more data is being created, extracted, analyzed and utilized. As individuals we see this as we go through our social media accounts, as we come face to face with recommender systems on our streaming services, as we try to avail of social services from the government, more data is asked of us and more data is being used upon us as well. The introduction of algorithms then exacerbates this further at the different scale. We're now developing algorithms, we're now utilizing machine learning that is based precisely on these huge amount of data that we are producing and that we are collecting. The intersection between these data and algorithms then lead to what we are now considering as data-driven practices. So we see, for example, in hospitals, we're in triage or decisions in terms of who gets to be treated first, is now being run through or being based on historical data, or decisions, for example, in other countries, not specifically ours at this particular juncture, with regards to policing or profiling is also based on data that has gone through algorithms and is now informing decisions. And the crux of the tension here is actually the seeming promise of these data-driven decisions is that there's a certain level of neutrality or rather bias that used to be with the agency of the person. And so to be data-driven in terms of our practices actually takes away or purports to take away the agency from the human and rather gives the decision or have decisions be driven by the data. And we don't need to unpack all of these, but any person specifically from, hopefully with your education from UP, would know that this is as much as it presents promise for innovation and progress, for efficiency and reach. There's also great threats and risks to our social norms, social activities, and at times can actually perpetuate existing and underlying social inequalities as well. And thus, because of the ubiquity and the interaction between these three, we then now more than ever really need to start talking about thinking through and practicing data ethics. So what are some three key actions that I would found at this entire discussion or rather would shape the exploration of what data ethics is and how we can go about pursuing it? So these three actions are drawn from Lisa Gittleman's introduction to the edited book, Raw Data is an Oxymoron, which I highly, highly recommend that all of you get a copy of. So let's go through them one by one. The first one being data are never raw. So I think every now and then when you're, rather when you're introduced to data science or data work, if ever you come face to face with the data set, we would often talk about raw data. The problem with the language use or calling raw data as raw data is, well, precisely it's an oxymoron because there is no such thing as raw data. So raw data assumes or rather asserts that there are certain forms of data that are quote unquote natural, that are untouched, that are not yet processed. But the fact of the matter is every data is actually generated and mediated by specific processes. The technologies that you use in terms of extracting data, the categorizations and the properties that you decide to pursue in your data collection is also processed, right? The sample size and the population that you're collecting data from is already a mechanism of certain social, technical and cognitive processes as well. And therefore data are never raw. And precisely because data are never raw, then that means that data are never neutral because it goes through specific mediated processes. We always say that data is powerful. If we believe that data is powerful, then by default it is always implicated in politics, right? So data never exists in a particular bubble, but rather it has always implicated in an interaction between communities, norms, laws, legalities, and of course, the technologies that are being deployed as well. And so data are never neutral. And data are never neutral precisely because data have context. Data are not just abstractions, but rather they are always linked and connected to relationships. Data represents people or data represents actions. Data represents time, data represents history. Whatever forms of data you are working with, it will always have a context and it always would be related to the ecosystem that it exists in. An important addition to this is that data is always temporal, meaning that data will always be defined or always be shaped by certain registers of temporality. For example, if you're collecting data, that data is collected at a particular time, and we can't take away that temporal context from that specific data that you got. But at the same time, data has a temporal relationship precisely because the data that you collect now invariably would have an impact both in terms of short-term, mid-term, and also long-term consequences as well. So if you look at these three actions together, in addition to the impetus that I just articulated, then the rationale or rather the call to talk about and pursue data ethics and how to actually go about them would be truly informed. There are numerous, of course, definitions of what data is and what ethics is. I'm not gonna go through that precisely because even the act of defining data ethics is both unwieldy and political, right? But that said, we're gonna go through a number of case studies and a number of approaches that points to how different communities are rather articulating or shaping what data ethics is and what data ethics can be. But I do want to zero in on one definition offered by Alex Hanna. Alex Hanna is currently the Director of Research for the Distributed AI Research Institute, but she is also a former senior research scientist under the Ethical AI Division of Google. She considers data ethics or rather the scope of data ethics to not just be about minimizing harm, harms that we just pointed out to, but also primarily how do we go about improving the lives of people by using data? I added here the notion of like, or not using data, which I think is as important as figuring out how to use data is actually when to not use data as well. Interestingly enough, Alex Hanna actually left Google a couple of months ago or maybe a little over a year ago, precisely because after working for Google, realized that even the tools and the technologies being employed by Google actually goes against the data ethics that she purports to stand for as well. So that in itself is interesting. So I'll take a little break here and show you a video, which I just realized I have to make sure. Data science is used in every field imaginable from marketing to medicine from- Yeah, I'm gonna stop here. And then I have to show, share my screen and share sound. There we go. Hopefully this works. Data science is used in every field imaginable from marketing to medicine, from transportation to waste management. And while a data scientist might feel a bit removed from the real world implementations of their work, their models and analytics will eventually affect real lives. The decisions we make because it's convenient or tidy or because we aren't entirely sure what the model is doing, yes, that will happen to you, can end up with large scale harms. Let's work through an example of a not so obvious mistake in an algorithm that snowballs into unjustly harming a lot of people. Risk assessment scores for whether or not someone is likely to commit a crime. This isn't science fiction, this is real life. Risk assessment scores are used in the criminal justice system today. Because people tend to see numbers and statistics as more objective, society has introduced predictive algorithms into our courtrooms and law enforcement agencies. It's not just one algorithm, it's many, ranging from predicting where a crime will occur or using someone's social media data to predict if they are likely to be more violent. And in this example, we will be looking at an algorithmic risk assessment of recidivism. Recidivism, the tendency of a convicted criminal to re-offend. Parole or release from prison with some restrictions can be granted to prisoners who are deemed fit to reintegrate into society. An individual may go up for parole where several factors will be examined by a judge, including their original crime, their behavior while serving in prison, their demographics and other features. With an overworked system and the desire to reduce imprisonment numbers, it makes sense to want an automated system to help determine who should get parole. But hidden in this desire for efficiency is the perpetuation of racial bias. Biases that have a lot of historical inertia in these systems. It is well known that our prison system is particularly harsh on Black and Brown members of society. The school to prison pipeline describes the disproportionate tendency of minors and young adults from disadvantaged backgrounds to become incarcerated because of increasingly harsh school and municipal policies, as well as because of educational inequality in the United States, with Black students affected the most harshly. Black men are at the highest risk for police violence and overall receive longer prison sentences than White men arrested for the same crimes. These racial disparities are present in all historical crime data used to train algorithms. Maybe you're seeing the problem now. A good way to repeat biases of the past would be to use them to train an automated tool to determine the future. When training data contains racial bias, the model will learn that racial bias and replicate it. We are telling a model, this is who got parole in the past, please automatically sort people to fit what we have done before. But what we have done before is racist. What seems objective and flawless to a judge or jury who are unlikely to know what went into the tool is actually an opaque replicator of bias. It is our job as data scientists to both think critically about algorithmic design and to communicate how algorithms work to non-experts. When in doubt, ask the stakeholders of the models to weigh in. We call this situated data science, where the goal is not to design for, but to design with. Remember to stay kind, stay curious, and stay critical. Data science is...